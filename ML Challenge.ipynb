{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyeeg\n",
    "import scipy.signal\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importer les données\n",
    "Les données sont sous la forme h5. Il faut donc les importer puis créer x dataset en fonction du nombre de dataset présents dans le fichier (10). On utilise ici r+ pour avoir droit de lecture et d'écriture.\n",
    "\n",
    "Chaque Dataset représente les mesures des accéléromètres, eeg et l'oximeter pour 38289 différentes mesures.\n",
    "Les catégories de sommeil sont des entiers entre 0 et 5 présentés dans un csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = h5py.File('train.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_csv('train_y.csv').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liste des données et extraction \n",
    "\n",
    "Voici la liste des jeux de données disponible et leur extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accelerometer_x', 'accelerometer_y', 'accelerometer_z', 'eeg_1', 'eeg_2', 'eeg_3', 'eeg_4', 'eeg_5', 'eeg_6', 'eeg_7', 'pulse_oximeter_infrared']\n"
     ]
    }
   ],
   "source": [
    "print(list(f.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.flush()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsetx = f['accelerometer_x']\n",
    "dsety = f['accelerometer_y']\n",
    "dsetz = f['accelerometer_z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dsetx4 = dsetx[y_test[:,1]==4,:]\n",
    "dsety4 = dsety[y_test[:,1]==4,:]\n",
    "dsetz4 = dsetz[y_test[:,1]==4,:]\n",
    "\n",
    "name=1\n",
    "for i in [dsetx4,dsety4,dsetz4] :\n",
    "    plt.scatter([i for i in range (len(i[1,:]))],i[1,:])\n",
    "    plt.savefig('acc_'+str(name)+'.png')\n",
    "    plt.clf()\n",
    "    name+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(h5,list_data_name, output):\n",
    "    for dset_name in list_data_name :\n",
    "        dset=h5[dset_name]\n",
    "        dset2=dset[y_test[:,1]==output,:]\n",
    "        plt.scatter([i for i in range (len(dset2[1,:]))],dset2[1,:])\n",
    "        plt.savefig('output_'+str(output)+dset_name+'.png')\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(1,5):\n",
    "    plotting(f,['accelerometer_x', 'accelerometer_y', 'accelerometer_z'],j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Power spectral Intensity          bin_power()\n",
    "petrosian Fractal Dimension       pdf()\n",
    "Higucho Fractal Dimension         hdf()\n",
    "Hjorth mobility and complexity    hjorth()\n",
    "Spectral Entrocpy (Shannon)       spectral_entropy()\n",
    "SVD Entropy                       svd_entropy()\n",
    "Fisher information                fisher_info()\n",
    "Approximate entropy               ap_entropy()\n",
    "Detrended Fluctuation Analysis    dfa()\n",
    "Hurst Exponent                    hurst() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a Vector with the indices\n",
    "\n",
    "numpy.apply_along_axis(func1d, axis = 1, arr, *args, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_mean(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(np.mean,1,np.apply_along_axis(abs,1,Dset_int))\n",
    "        Features[str(i)+'_abs_mean']=Col\n",
    "        \n",
    "        \n",
    "    return Features\n",
    "\n",
    "def mean(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(np.mean,1,Dset_int)\n",
    "        Features[str(i)+'_mean']=Col\n",
    "    return Features\n",
    "\n",
    "def max_value(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(max,1,Dset_int)\n",
    "        Features[str(i)+'_max_value']=Col\n",
    "        \n",
    "    return Features\n",
    "\n",
    "\n",
    "def min_value(Features, lst_data, dset) :        \n",
    "    \n",
    "    for i in lst_data :\n",
    "        \n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(min,1,Dset_int)\n",
    "        Features[str(i)+'_min_value']=Col\n",
    "        \n",
    "    return Features\n",
    "\n",
    "\n",
    "def max_abs_value(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(max,1,np.apply_along_axis(abs,1,Dset_int))\n",
    "        Features[str(i)+'_max_abs_value']=Col\n",
    "        \n",
    "    return Features\n",
    "\n",
    "\n",
    "def abs_mean_derivate(Features, lst_data, dset):\n",
    "   \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        multiplicative_coef = len(Dset_int[1])/30\n",
    "        Col = np.apply_along_axis(np.mean,1,np.apply_along_axis(abs,1,np.apply_along_axis(np.gradient,1,Dset_int)*multiplicative_coef))\n",
    "        Features[str(i)+'_abs_mean_derivate']=Col\n",
    "        \n",
    "    return Features\n",
    "\n",
    "def max_abs_derivate(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        multiplicative_coef = len(Dset_int[1])/30\n",
    "        Col = np.apply_along_axis(max,1,np.apply_along_axis(abs,1,np.apply_along_axis(np.gradient,1,Dset_int)*multiplicative_coef))\n",
    "        Features[str(i)+'_max_abs_derivate']=Col\n",
    "        \n",
    "    return Features\n",
    "\n",
    "\n",
    "def max_value_derivate(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        multiplicative_coef = len(Dset_int[1])/30\n",
    "        Col = np.apply_along_axis(max,1,np.apply_along_axis(np.gradient,1,Dset_int)*multiplicative_coef)\n",
    "        Features[str(i)+'_max_value_derivate']=Col\n",
    "       \n",
    "    return Features\n",
    "\n",
    "\n",
    "def min_value_derivate(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        multiplicative_coef = len(Dset_int[1])/30\n",
    "        Col = np.apply_along_axis(min,1,np.apply_along_axis(np.gradient,1,Dset_int)*multiplicative_coef)\n",
    "        Features[str(i)+'_min_value_derivate']=Col\n",
    "        \n",
    "    return Features\n",
    "\n",
    "\n",
    "def freq_max_power(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(max,1,(np.apply_along_axis(abs,1,np.apply_along_axis(np.fft.fft,1,Dset_int))))\n",
    "        Features[str(i)+'_freq_max_power']=Col\n",
    "       \n",
    "    return Features\n",
    "\n",
    "\n",
    "def freq_max_value(Features, lst_data, dset):\n",
    "    \n",
    "    \n",
    "    for i in  lst_data:\n",
    "        Dset_int=dset[i]\n",
    "        Col = pd.DataFrame(index = np.arange(Dset_int.shape[0]))\n",
    "        Col['freq'] = \"\"\n",
    "        FFTf = np.fft.fftfreq(len(Dset_int[1]))*len(Dset_int[1])/30\n",
    "        FFTf = pd.DataFrame(FFTf)\n",
    "        FFTi =np.apply_along_axis(np.argmax,1,( np.apply_along_axis(abs,1,np.apply_along_axis(np.fft.fft,1,Dset_int))))\n",
    "        FFTi = pd.DataFrame(FFTi)\n",
    "        for j in range(Dset_int.shape[0]) : \n",
    "            Col.iloc[j,0] = FFTf.iloc[FFTi.iloc[j,0],0]\n",
    "\n",
    "        Features[str(i)+'_freq_max_value'] = Col  \n",
    "        \n",
    "    return Features\n",
    "\n",
    "\n",
    "def max_amplitude_fft(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(max,1,(np.apply_along_axis(abs,1,np.apply_along_axis(np.fft.fft,1,Dset_int))))\n",
    "        Features[str(i)+'_max_amplitude_fft']=Col\n",
    "       \n",
    "    return Features\n",
    "\n",
    "#MAX_POWER = max_amplitude_fft(Features, lst_data, dset)\n",
    "\n",
    "def freq_max_amplitude_fft(Features, lst_data, dset):\n",
    "    \n",
    "    \n",
    "    for i in  lst_data:\n",
    "        Dset_int=dset[i]\n",
    "        Col = pd.DataFrame(index = np.arange(Dset_int.shape[0]))\n",
    "        Col['freq'] = \"\"\n",
    "        FFTf = np.fft.fftfreq(len(Dset_int[1]))*len(Dset_int[1])/30\n",
    "        FFTf = pd.DataFrame(FFTf)\n",
    "        FFTi =np.apply_along_axis(np.argmax,1,( np.apply_along_axis(abs,1,np.apply_along_axis(np.fft.fft,1,Dset_int))))\n",
    "        FFTi = pd.DataFrame(FFTi)\n",
    "        for j in range(Dset_int.shape[0]) : \n",
    "            Col.iloc[j,0] = FFTf.iloc[abs(FFTi.iloc[j,0]),0]\n",
    "\n",
    "        Features[str(i)+'_freq_max_amplitude_fft'] = Col  \n",
    "        \n",
    "    return Features\n",
    "\n",
    "def mean_amplitude_fft(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(np.mean,1,(np.apply_along_axis(abs,1,np.apply_along_axis(np.fft.fft,1,Dset_int))))\n",
    "        Features[str(i)+'_mean_amplitude_fft']=Col\n",
    "       \n",
    "    return Features\n",
    "\n",
    "def peak(Features, lst_data, dset):\n",
    "    \n",
    "    for i in  lst_data:\n",
    "        \n",
    "        Dset_int=dset[i]\n",
    "        Tableau_signal = pd.DataFrame(Dset_int[:,:])\n",
    "        Col = pd.DataFrame(index = np.arange(Dset_int.shape[0]))\n",
    "        Col['peak'] = \"\"\n",
    "        DF = np.apply_along_axis(scipy.signal.find_peaks,1,np.apply_along_axis(abs,1,Dset_int),distance = Dset_int.shape[1])\n",
    "        DF = pd.DataFrame(DF)\n",
    "        DF = DF[0]\n",
    "        DF = DF.apply(int)\n",
    "        \n",
    "        for j in range(Dset_int.shape[0]) : \n",
    "            Col.iloc[j,0] = Tableau_signal.iloc[j,DF.iloc[j]]/np.mean(abs(Tableau_signal.iloc[j,:]))\n",
    "        \n",
    "        Features[str(i)+'_peak'] = Col   \n",
    "        \n",
    "    return Features\n",
    "\n",
    "def puissance_moy_periodogram(Features, lst_data, dset) : \n",
    "   \n",
    "     \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Periodrogram_array =np.apply_along_axis(scipy.signal.periodogram,1,Dset_int)\n",
    "        Periodrogram_array = Periodrogram_array[:,1,:] \n",
    "        Col = np.apply_along_axis(np.mean,1,Periodrogram_array)\n",
    "        Features[str(i)+'__mean_power_periodogram']=Col\n",
    "       \n",
    "    return Features\n",
    " \n",
    "def freq_max_power_periodogram(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Periodrogram_array =np.apply_along_axis(scipy.signal.periodogram,1,Dset_int)\n",
    "        Periodrogram_array = Periodrogram_array[:,1,:] \n",
    "        Col = np.apply_along_axis(max,1,Periodrogram_array)\n",
    "        Features[str(i)+'_freq_max_power_periodogram']=Col\n",
    "       \n",
    "    return Features\n",
    "\n",
    "def freq_max_value_periodogram(Features, lst_data, dset):\n",
    "    \n",
    "    for i in  lst_data:\n",
    "        \n",
    "        Dset_int=dset[i]\n",
    "        Col = pd.DataFrame(index = np.arange(Dset_int.shape[0]))\n",
    "        Col['freq'] = \"\"\n",
    "        Periodrogram_array = np.apply_along_axis(scipy.signal.periodogram,1,Dset_int)\n",
    "        Periodogram_frequences = Periodrogram_array[1,0,:]\n",
    "        Periodogram_frequences = pd.DataFrame(Periodogram_frequences)\n",
    "        Periodrogram_array = Periodrogram_array[:,1,:] \n",
    "        Periodrogram_array = np.apply_along_axis(np.argmax,1,Periodrogram_array)\n",
    "        Periodrogram_array = pd.DataFrame(Periodrogram_array)\n",
    "        \n",
    "        for j in range(Dset_int.shape[0]) : \n",
    "            Col.iloc[j,0] = Periodogram_frequences.iloc[abs(Periodrogram_array.iloc[j,0]),0]\n",
    "    \n",
    "        Features[str(i)+'_freq_max_value_periodrogram'] = Col  \n",
    "        \n",
    "    return Features\n",
    " \n",
    "#Mean_POWER = mean_amplitude_fft(Features, lst_data, dset)\n",
    "\n",
    "#Bin frequences\n",
    "\n",
    "def bin_power(X, Band, Fs):\n",
    "    \n",
    "    \"\"\"Compute power in each frequency bin specified by Band from FFT result of\n",
    "    X. By default, X is a real signal.\n",
    "    Note\n",
    "    -----\n",
    "    A real signal can be synthesized, thus not real.\n",
    "    Parameters\n",
    "    -----------\n",
    "    Band\n",
    "        list\n",
    "        boundary frequencies (in Hz) of bins. They can be unequal bins, e.g.\n",
    "        [0.5,4,7,12,30] which are delta, theta, alpha and beta respectively.\n",
    "        You can also use range() function of Python to generate equal bins and\n",
    "        pass the generated list to this function.\n",
    "        Each element of Band is a physical frequency and shall not exceed the\n",
    "        Nyquist frequency, i.e., half of sampling frequency.\n",
    "     X\n",
    "        list\n",
    "        a 1-D real time series.\n",
    "    Fs\n",
    "        integer\n",
    "        the sampling rate in physical frequency\n",
    "    Returns\n",
    "    -------\n",
    "    Power\n",
    "        list\n",
    "        spectral power in each frequency bin.\n",
    "    Power_ratio\n",
    "        list\n",
    "        spectral power in each frequency bin normalized by total power in ALL\n",
    "        frequency bins.\n",
    "    \"\"\"\n",
    "\n",
    "    C = np.fft.fft(X)\n",
    "    C = abs(C)\n",
    "    Power = np.zeros(len(Band) - 1)\n",
    "    for Freq_Index in range(0, len(Band) - 1):\n",
    "        Freq = float(Band[Freq_Index])\n",
    "        Next_Freq = float(Band[Freq_Index + 1])\n",
    "        Power[Freq_Index] = np.sum(\n",
    "            C[int(Freq / Fs * len(X)) : int(Next_Freq / Fs * len(X))]\n",
    "        )\n",
    "    Power_Ratio = Power / sum(Power)\n",
    "    return Power, Power_Ratio\n",
    "\n",
    "\n",
    "def bin_power_features(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Resultat_int = np.apply_along_axis(bin_power,1,Dset_int, Band = [0.5,4,7,12,30], Fs = len(Dset_int[1])/30)\n",
    "        Array_somme_frequence = Resultat_int[:,0,:]\n",
    "        Array_somme_frequence = pd.DataFrame(Array_somme_frequence)\n",
    "        Array_somme_frequence.columns = ['Delta','Theta','Alpha','Beta']\n",
    "        Array_somme_frequence_normalisee = Resultat_int[:,1,:]\n",
    "        Array_somme_frequence_normalisee = pd.DataFrame(Array_somme_frequence_normalisee)\n",
    "        Array_somme_frequence_normalisee.columns = ['Delta_N','Theta_N','Alpha_N','Beta_N']\n",
    "        Features[str(i)+'Delta'] = Array_somme_frequence['Delta']\n",
    "        Features[str(i)+'Theta'] = Array_somme_frequence['Theta']\n",
    "        Features[str(i)+'Alpha'] = Array_somme_frequence['Alpha']\n",
    "        Features[str(i)+'Beta'] = Array_somme_frequence['Beta']\n",
    "        Features[str(i)+'Delta_N'] = Array_somme_frequence_normalisee['Delta_N']\n",
    "        Features[str(i)+'Theta_N'] = Array_somme_frequence_normalisee['Theta_N']\n",
    "        Features[str(i)+'Alpha_N'] = Array_somme_frequence_normalisee['Alpha_N']\n",
    "        Features[str(i)+'Beta_N'] = Array_somme_frequence_normalisee['Beta_N']\n",
    "       \n",
    "    return Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal analysis\n",
    "Pyeeg module \n",
    "Goal : créer des fonctions pour extraire pour un dataset un certain features. \n",
    "On pourra ainsi itérer pour plusieurs fonctions afin d'obtenir nos features pour chaque input.\n",
    "On stocke les valeurs dans un Dataset (Features) afin de garder une tracabilité des colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fish_info_feat(Features, lst_data, dset):\n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col=np.apply_along_axis(lambda x : pyeeg.fisher_info(x,1,4),1, Dset_int)\n",
    "        Features[str(i)+'_fish_info']=Col\n",
    "    return Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features=fish_info_feat(Features, ['eeg_1','eeg_2'],f)\n",
    "Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extraction on a Dataset\n",
    "\n",
    "Function qui prend un H5 en entrée, et qui lui applique toutes nos fonctions d'extraction de Features. Il retourne un DataFrame contenant tous les Features extraits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features=pd.DataFrame()\n",
    "all_dset=list(f.keys())\n",
    "col_eeg_oxy=['eeg_1', 'eeg_2', 'eeg_3', 'eeg_4', 'eeg_5', 'eeg_6', 'eeg_7', 'pulse_oximeter_infrared']\n",
    "col_acc=['accelerometer_x', 'accelerometer_y', 'accelerometer_z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features=abs_mean(Features,all_dset,dset)\n",
    "Features=max_value(Features,all_dset,dset)\n",
    "Features=min_value(Features,all_dset,dset)\n",
    "Features=max_abs_value(Features,all_dset,dset)\n",
    "Features=abs_mean_derivate(Features,all_dset,dset)\n",
    "Features=max_abs_derivate(Features,all_dset,dset)\n",
    "Features=max_value_derivate(Features,all_dset,dset)\n",
    "Features=min_value_derivate(Features,all_dset,dset)\n",
    "Features=freq_max_power(Features,all_dset,dset)\n",
    "Features=freq_max_value(Features,all_dset,dset)\n",
    "Features=peak(Features,all_dset,dset)\n",
    "Features=fish_info_feat(Features,col_eeg_oxy,dset)\n",
    "Features= bin_power_features(Features, lst_data, dset)\n",
    "Features=freq_max_power_periodogram(Features, lst_data, dset)\n",
    "Features=puissance_moy_periodogram(Features, lst_data, dset)\n",
    "Features=max_amplitude_fft(Features, lst_data, dset)\n",
    "Features=freq_max_amplitude_fft(Features, lst_data, dset)\n",
    "Features=mean_amplitude_fft(Features, lst_data, dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Testing\n",
    "\n",
    "Ensemble de cellules permettant d'entrainer le modèle sur une partie du Dataset pour le tester sur une autre .\n",
    "\n",
    "Afin de créer le Dataset, il y a deux possibilités : lire un csv ou recréer la Base de données\n",
    "\n",
    "Voir cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_train = pd.read_csv('Features_train_2.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('train_y.csv').values[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Features_train = Features_train.iloc[:,1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Features_train.values, y, test_size = 0.25, random_state = 0)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       [0., 0., 0., 1., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., 0., 1.],\n",
       "       [0., 0., 1., 0., 0.],\n",
       "       [0., 0., 0., 1., 0.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_mult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multilabel-indicator is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-2070999be2ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_m\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: multilabel-indicator is not supported"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    " \n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<28716x5 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 28716 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6083777290295623"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test,y_test_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 593,   22,  176,   14,   95],\n",
       "       [  69,   20,  141,    8,   76],\n",
       "       [ 127,   37, 3506,  191,  454],\n",
       "       [  30,   11,  325, 1048,   26],\n",
       "       [  77,   28,  629,   29, 1841]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "classifier = XGBClassifier(max_depth=50, n_estimators=10, n_jobs=-1)\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7320589157004074"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "X=Features_train.values\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X=sc.fit_transform(X)\n",
    "cluster = KMeans(n_clusters=5, init='k-means++', max_iter=100)\n",
    "y_pred=cluster.fit_predict(X,y)\n",
    "Features_train['y'] = y\n",
    "Features_train['pred'] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "Clust_comp = Features_train.iloc[:,-3:].groupby(['y','pred']).count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>pulse_oximeter_infrared__mean_power_periodogram</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>y</th>\n",
       "      <th>pred</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">0</th>\n",
       "      <th>0</th>\n",
       "      <td>1391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>30</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>117</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">1</th>\n",
       "      <th>0</th>\n",
       "      <td>609</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>181</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">2</th>\n",
       "      <th>0</th>\n",
       "      <td>10927</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"4\" valign=\"top\">3</th>\n",
       "      <th>0</th>\n",
       "      <td>1625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>67</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">4</th>\n",
       "      <th>0</th>\n",
       "      <td>7413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>224</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>47</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        pulse_oximeter_infrared__mean_power_periodogram\n",
       "y pred                                                 \n",
       "0 0                                                1391\n",
       "  1                                                  30\n",
       "  2                                                 890\n",
       "  3                                                1191\n",
       "  4                                                 117\n",
       "1 0                                                 609\n",
       "  2                                                 181\n",
       "  3                                                 551\n",
       "  4                                                  12\n",
       "2 0                                               10927\n",
       "  1                                                   5\n",
       "  2                                                 263\n",
       "  3                                                5865\n",
       "  4                                                  72\n",
       "3 0                                                1625\n",
       "  2                                                  67\n",
       "  3                                                4035\n",
       "  4                                                   4\n",
       "4 0                                                7413\n",
       "  1                                                   3\n",
       "  2                                                 224\n",
       "  3                                                2767\n",
       "  4                                                  47"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Clust_comp()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[y_pred==1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_cat(y_cat) : \n",
    "    y = y_cat\n",
    "    y[y == 1] = 0\n",
    "    y[y == 3] = 0\n",
    "    y[y == 2] = 1\n",
    "    y[y == 4] = 1\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_doub = bin_cat(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1750,  904],\n",
       "       [ 376, 6543]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Features_train.values, y_doub, test_size = 0.25, random_state = 0)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8662906090044918"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = model.predict(Features_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_train['y_pred_1'] = y_pred_1\n",
    "Features_train['y'] = y\n",
    "Train_1 = Features_train.loc[Features_train['y_pred_1']==1,:]\n",
    "Train_0 = Features_train.loc[Features_train['y_pred_1']==0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 59,  50],\n",
       "       [ 28, 156]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RD Forest train0\n",
    "\n",
    "X_train_0, X_test_0, y_train_0, y_test_0 = train_test_split(Train_0.iloc[:,:-2].values,Train_0.iloc[:,-1], test_size = 0.25, random_state = 0)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train_0=sc.fit_transform(X_train_0)\n",
    "X_test_0=sc.transform(X_test_0)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_0 = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model_0.fit(X_train_0,y_train_0)\n",
    "y_pred_0 = model.predict(X_test_0)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_0 = confusion_matrix(y_test_0, y_pred_0)\n",
    "cm_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7337883959044369"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test_0,y_test_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1738,  875],\n",
       "       [ 613, 6054]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RD Forest train0\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(Train_1.iloc[:,:-2].values,Train_1.iloc[:,-1], test_size = 0.25, random_state = 0)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train_1=sc.fit_transform(X_train_1)\n",
    "X_test_1=sc.transform(X_test_1)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_1 = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model_1.fit(X_train_1,y_train_1)\n",
    "y_pred_1 = model.predict(X_test_1)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_1 = confusion_matrix(y_test_1, y_pred_1)\n",
    "cm_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8671336206896552"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.score(X_test_1,y_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 4106,  6597],\n",
       "       [ 8631, 18955]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_0['y_pred_f'] = model_0.predict(Train_0.iloc[:,:-2].values)\n",
    "Train_1['y_pred_f'] = model_1.predict(Train_1.iloc[:,:-2].values)\n",
    "\n",
    "Res = pd.concat([Train_0,Train_1])\n",
    "cm_f = confusion_matrix(Res['y'],Res['y_pred_f'])\n",
    "cm_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.602692548528491"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(19000+4100)/(8631+6597+19000+4100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "Feat_tot=Features_train\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_kfold = StandardScaler()\n",
    "Feat_tot = sc_kfold.fit_transform(Feat_tot)\n",
    "\n",
    "## Model \n",
    "classifier = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "\n",
    "scores = cross_val_score(classifier, Feat_tot, y, cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Random Forrest + one Hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.62 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(categorical_features=[0])\n",
    "y_mult = ohe.fit_transform(y.reshape((-1,1))).toarray()\n",
    "\n",
    "Feat_tot=Features_train\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_kfold = StandardScaler()\n",
    "Feat_tot = sc_kfold.fit_transform(Feat_tot)\n",
    "\n",
    "## Model \n",
    "classifier = RandomForestClassifier(n_estimators=100, n_jobs=-1,)\n",
    "\n",
    "scores = cross_val_score(classifier, Feat_tot, y_mult, cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.64 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "Feat_tot=Features_train\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_kfold = StandardScaler()\n",
    "Feat_tot = sc_kfold.fit_transform(Feat_tot)\n",
    "\n",
    "## Model \n",
    "classifier = XGBClassifier(max_depth=5, n_estimators=100, n_jobs=-1)\n",
    "\n",
    "scores = cross_val_score(classifier, Feat_tot, y, cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training sans test\n",
    "\n",
    "Ensemble de cellules permettant d'entrainer le modèle sur  Dataset pour le tester sur une autre \n",
    "\n",
    "Voir cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_pred = h5py.File('test.h5', 'r')\n",
    "y_train=pd.read_csv('train_y.csv').values[:,1]\n",
    "dset = h5py.File('train.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dset=list(dset.keys())\n",
    "col_eeg_oxy=['eeg_1', 'eeg_2', 'eeg_3', 'eeg_4', 'eeg_5', 'eeg_6', 'eeg_7', 'pulse_oximeter_infrared']\n",
    "col_acc=['accelerometer_x', 'accelerometer_y', 'accelerometer_z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'scipy' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-56d62d17002a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mFeatures_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfreq_max_power\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFeatures_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall_dset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mFeatures_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfreq_max_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFeatures_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall_dset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mFeatures_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpeak\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFeatures_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mall_dset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mFeatures_train\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfish_info_feat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mFeatures_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcol_eeg_oxy\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-19-dfb7445c02b9>\u001b[0m in \u001b[0;36mpeak\u001b[1;34m(Features, lst_data, dset)\u001b[0m\n\u001b[0;32m    128\u001b[0m         \u001b[0mCol\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDset_int\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m         \u001b[0mCol\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'peak'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m         \u001b[0mDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscipy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignal\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_peaks\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_along_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDset_int\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdistance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDset_int\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m         \u001b[0mDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDF\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[0mDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDF\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'scipy' is not defined"
     ]
    }
   ],
   "source": [
    "Features_train=pd.DataFrame()\n",
    "Features_train=abs_mean(Features_train,all_dset,dset)\n",
    "Features_train=max_value(Features_train,all_dset,dset)\n",
    "Features_train=min_value(Features_train,all_dset,dset)\n",
    "Features_train=max_abs_value(Features_train,all_dset,dset)\n",
    "Features_train=abs_mean_derivate(Features_train,all_dset,dset)\n",
    "Features_train=max_abs_derivate(Features_train,all_dset,dset)\n",
    "Features_train=max_value_derivate(Features_train,all_dset,dset)\n",
    "Features_train=min_value_derivate(Features_train,all_dset,dset)\n",
    "Features_train=freq_max_power(Features_train,all_dset,dset)\n",
    "Features_train=freq_max_value(Features_train,all_dset,dset)\n",
    "Features_train=peak(Features_train,all_dset,dset)\n",
    "Features_train=fish_info_feat(Features_train,col_eeg_oxy,dset)\n",
    "## ADD Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_train.to_csv('Features_train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "1\n",
      "4272.845386505127\n"
     ]
    }
   ],
   "source": [
    "start=time.time()\n",
    "Features_test=pd.DataFrame()\n",
    "print(1)\n",
    "Features_test=abs_mean(Features_test,all_dset,dset_pred)\n",
    "print(1)\n",
    "Features_test=max_value(Features_test,all_dset,dset_pred)\n",
    "print(1)\n",
    "Features_test=min_value(Features_test,all_dset,dset_pred)\n",
    "print(1)\n",
    "Features_test=max_abs_value(Features_test,all_dset,dset_pred)\n",
    "print(1)\n",
    "Features_test=abs_mean_derivate(Features_test,all_dset,dset_pred)\n",
    "print(1)\n",
    "Features_test=max_abs_derivate(Features_test,all_dset,dset_pred)\n",
    "print(1)\n",
    "Features_test=max_value_derivate(Features_test,all_dset,dset_pred)\n",
    "print(1)\n",
    "Features_test=min_value_derivate(Features_test,all_dset,dset_pred)\n",
    "print(1)\n",
    "Features_test=freq_max_power(Features_test,all_dset,dset_pred)\n",
    "print(1)\n",
    "Features_test=freq_max_value(Features_test,all_dset,dset_pred)\n",
    "print(1)\n",
    "Features_test=peak(Features_test,all_dset,dset_pred)\n",
    "print(1)\n",
    "Features_test=fish_info_feat(Features_test,col_eeg_oxy,dset_pred)\n",
    "end=time.time()\n",
    "print(end-start)\n",
    "#ADD Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_test.to_csv('Features_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From already preprocessed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_train = pd.read_csv('Features_train_2.csv')\n",
    "Features_test = pd.read_csv('Features_test_2.csv')\n",
    "y_train = pd.read_csv('train_y.csv').values[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = Features_train.loc[:,:].values\n",
    "X_pred = Features_test.loc[:,:].values\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_pred = sc.transform(X_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model training\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_pred)\n",
    "Res=pd.DataFrame()\n",
    "Res['sleep_stage']=y_pred\n",
    "Res.index.name='id'\n",
    "Res.to_csv('y_pred.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=50, min_child_weight=1, missing=None, n_estimators=25,\n",
       "       n_jobs=-1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier_xg = XGBClassifier(max_depth=50, n_estimators=25, n_jobs=-1)\n",
    "classifier_xg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier_xg.predict(X_pred)\n",
    "Res=pd.DataFrame()\n",
    "Res['sleep_stage']=y_pred\n",
    "Res.index.name='id'\n",
    "Res.to_csv('y_pred_xg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "frequency=500\n",
    "duration =5000\n",
    "winsound.Beep(frequency,duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Random_forrest_Dreem.joblib']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(model, 'Random_forrest_Dreem.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Threshold ML\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_train = pd.read_csv('Features_train_2.csv') \n",
    "Features_train = Features_train.iloc[:,1:]\n",
    "Features_test = pd.read_csv('Features_test_2.csv')\n",
    "Features_test = Features_test.iloc[:,1:]\n",
    "y_train = pd.read_csv('train_y.csv').iloc[:,[1]] #DF (WITH index)\n",
    "y_train['y_cat'] = bin_cat(y_train.values.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating Sleep Stages 0,1,3 from 2,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = Features_train.loc[:,:].values\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_bin = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model_bin.fit(X_train,y_train.iloc[:,1].values)\n",
    "\n",
    "\n",
    "#y_train['y_pred_ts'] = y_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extrating Sleep stages 2,4 and categorizing them again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['sleep_stage', 'y_cat'], dtype='object')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1 = Features_train.loc[y_train['y_cat']==1,:]\n",
    "y_train_1 = y_train.loc[y_train['y_cat']==1,'sleep_stage'].values\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_2=StandardScaler()\n",
    "X_train_1=sc_2.fit_transform(X_train_1)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_2_4 = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model_2_4 .fit(X_train_1,y_train_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forrest on the whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2 = Features_train.loc[:,:].values\n",
    "y_train_2 = y_train.iloc[:,0].values\n",
    "X_train_2 = sc.transform(X_train_2)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_glob = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model_glob.fit(X_train_2,y_train_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-57-6d15198be014>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel_glob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'test.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "model_glob.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "X_test = Features_test.loc[:,:].values\n",
    "X_test = sc.transform(X_test)\n",
    "y_test = pd.DataFrame()\n",
    "y_test['y_glob'] = model_glob.predict(X_test)\n",
    "y_test['y_cat'] = model_bin.predict(X_test) \n",
    "\n",
    "\n",
    "\n",
    "X_test_1 = Features_test.loc[y_test['y_cat']==1,:]\n",
    "X_test_1=sc_2.transform(X_test_1)\n",
    "y_int = y_test.loc[y_test['y_cat']==1,['y_cat']]\n",
    "y_int['y_bin'] = model_2_4.predict(X_test_1)\n",
    "\n",
    "\n",
    "\n",
    "y_test = y_test.merge(y_int.loc[:,['y_bin']],right_index = True, left_index = True, how='outer')\n",
    "\n",
    "\n",
    "y_test['sleep_stage'] = y_test['y_bin'].fillna(y_test['y_glob'])\n",
    "\n",
    "y_test.index.name='id'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.loc[y_test['sleep_stage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test['sleep_stage'] = y_test['sleep_stage'].astype('int64')\n",
    "y_test.loc[:,['sleep_stage']].to_csv('y_pred_triple.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Can only tuple-index with a MultiIndex",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-31-06f01c429b95>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0my_int\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    808\u001b[0m             \u001b[0mkey\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    809\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 810\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    812\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_with\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    821\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 823\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_values_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    824\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    825\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36m_get_values_tuple\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    867\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMultiIndex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 868\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Can only tuple-index with a MultiIndex'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    869\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    870\u001b[0m         \u001b[1;31m# If key is contained, would have returned by now\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Can only tuple-index with a MultiIndex"
     ]
    }
   ],
   "source": [
    "y_int[:,5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing values for 2,4 by the second result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
