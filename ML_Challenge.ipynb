{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyeeg\n",
    "import scipy.signal\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importer les données\n",
    "Les données sont sous la forme h5. Il faut donc les importer puis créer x dataset en fonction du nombre de dataset présents dans le fichier (10). On utilise ici r+ pour avoir droit de lecture et d'écriture.\n",
    "\n",
    "Chaque Dataset représente les mesures des accéléromètres, eeg et l'oximeter pour 38289 différentes mesures.\n",
    "Les catégories de sommeil sont des entiers entre 0 et 5 présentés dans un csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = h5py.File('train.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_csv('train_y.csv').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liste des données et extraction \n",
    "\n",
    "Voici la liste des jeux de données disponible et leur extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsetx = f['accelerometer_x']\n",
    "dsety = f['accelerometer_y']\n",
    "dsetz = f['accelerometer_z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dsetx4 = dsetx[y_test[:,1]==4,:]\n",
    "dsety4 = dsety[y_test[:,1]==4,:]\n",
    "dsetz4 = dsetz[y_test[:,1]==4,:]\n",
    "\n",
    "name=1\n",
    "for i in [dsetx4,dsety4,dsetz4] :\n",
    "    plt.scatter([i for i in range (len(i[1,:]))],i[1,:])\n",
    "    plt.savefig('acc_'+str(name)+'.png')\n",
    "    plt.clf()\n",
    "    name+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(h5,list_data_name, output):\n",
    "    for dset_name in list_data_name :\n",
    "        dset=h5[dset_name]\n",
    "        dset2=dset[y_test[:,1]==output,:]\n",
    "        plt.scatter([i for i in range (len(dset2[1,:]))],dset2[1,:])\n",
    "        plt.savefig('output_'+str(output)+dset_name+'.png')\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(1,5):\n",
    "    plotting(f,['accelerometer_x', 'accelerometer_y', 'accelerometer_z'],j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'ensemble des fonctions suivantes permettent de processer les features qui nous intéressent. Chaque fonction prend en entrée un Dataframe de Features, la liste des eeg/acceleromètres à prendre en compte et le dataset entier.\n",
    "Elle renvoie le Dataframe Features avec les nouveaux Features ajoutés ou actualisés s'ils étaient déjà présent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_mean(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(np.mean,1,np.apply_along_axis(abs,1,Dset_int))\n",
    "        Features[str(i)+'_abs_mean']=Col\n",
    "        \n",
    "        \n",
    "    return Features\n",
    "\n",
    "def mean(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(np.mean,1,Dset_int)\n",
    "        Features[str(i)+'_mean']=Col\n",
    "    return Features\n",
    "\n",
    "def max_value(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(max,1,Dset_int)\n",
    "        Features[str(i)+'_max_value']=Col\n",
    "        \n",
    "    return Features\n",
    "\n",
    "\n",
    "def min_value(Features, lst_data, dset) :        \n",
    "    \n",
    "    for i in lst_data :\n",
    "        \n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(min,1,Dset_int)\n",
    "        Features[str(i)+'_min_value']=Col\n",
    "        \n",
    "    return Features\n",
    "\n",
    "\n",
    "def max_abs_value(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(max,1,np.apply_along_axis(abs,1,Dset_int))\n",
    "        Features[str(i)+'_max_abs_value']=Col\n",
    "        \n",
    "    return Features\n",
    "\n",
    "\n",
    "def abs_mean_derivate(Features, lst_data, dset):\n",
    "   \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        multiplicative_coef = len(Dset_int[1])/30\n",
    "        Col = np.apply_along_axis(np.mean,1,np.apply_along_axis(abs,1,np.apply_along_axis(np.gradient,1,Dset_int)*multiplicative_coef))\n",
    "        Features[str(i)+'_abs_mean_derivate']=Col\n",
    "        \n",
    "    return Features\n",
    "\n",
    "def max_abs_derivate(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        multiplicative_coef = len(Dset_int[1])/30\n",
    "        Col = np.apply_along_axis(max,1,np.apply_along_axis(abs,1,np.apply_along_axis(np.gradient,1,Dset_int)*multiplicative_coef))\n",
    "        Features[str(i)+'_max_abs_derivate']=Col\n",
    "        \n",
    "    return Features\n",
    "\n",
    "\n",
    "def max_value_derivate(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        multiplicative_coef = len(Dset_int[1])/30\n",
    "        Col = np.apply_along_axis(max,1,np.apply_along_axis(np.gradient,1,Dset_int)*multiplicative_coef)\n",
    "        Features[str(i)+'_max_value_derivate']=Col\n",
    "       \n",
    "    return Features\n",
    "\n",
    "\n",
    "def min_value_derivate(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        multiplicative_coef = len(Dset_int[1])/30\n",
    "        Col = np.apply_along_axis(min,1,np.apply_along_axis(np.gradient,1,Dset_int)*multiplicative_coef)\n",
    "        Features[str(i)+'_min_value_derivate']=Col\n",
    "        \n",
    "    return Features\n",
    "\n",
    "\n",
    "def freq_max_power(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(max,1,(np.apply_along_axis(abs,1,np.apply_along_axis(np.fft.fft,1,Dset_int))))\n",
    "        Features[str(i)+'_freq_max_power']=Col\n",
    "       \n",
    "    return Features\n",
    "\n",
    "\n",
    "def freq_max_value(Features, lst_data, dset):\n",
    "    \n",
    "    \n",
    "    for i in  lst_data:\n",
    "        Dset_int=dset[i]\n",
    "        Col = pd.DataFrame(index = np.arange(Dset_int.shape[0]))\n",
    "        Col['freq'] = \"\"\n",
    "        FFTf = np.fft.fftfreq(len(Dset_int[1]))*len(Dset_int[1])/30\n",
    "        FFTf = pd.DataFrame(FFTf)\n",
    "        FFTi =np.apply_along_axis(np.argmax,1,( np.apply_along_axis(abs,1,np.apply_along_axis(np.fft.fft,1,Dset_int))))\n",
    "        FFTi = pd.DataFrame(FFTi)\n",
    "        for j in range(Dset_int.shape[0]) : \n",
    "            Col.iloc[j,0] = FFTf.iloc[FFTi.iloc[j,0],0]\n",
    "\n",
    "        Features[str(i)+'_freq_max_value'] = Col  \n",
    "        \n",
    "    return Features\n",
    "\n",
    "\n",
    "def max_amplitude_fft(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(max,1,(np.apply_along_axis(abs,1,np.apply_along_axis(np.fft.fft,1,Dset_int))))\n",
    "        Features[str(i)+'_max_amplitude_fft']=Col\n",
    "       \n",
    "    return Features\n",
    "\n",
    "#MAX_POWER = max_amplitude_fft(Features, lst_data, dset)\n",
    "\n",
    "def freq_max_amplitude_fft(Features, lst_data, dset):\n",
    "    \n",
    "    \n",
    "    for i in  lst_data:\n",
    "        Dset_int=dset[i]\n",
    "        Col = pd.DataFrame(index = np.arange(Dset_int.shape[0]))\n",
    "        Col['freq'] = \"\"\n",
    "        FFTf = np.fft.fftfreq(len(Dset_int[1]))*len(Dset_int[1])/30\n",
    "        FFTf = pd.DataFrame(FFTf)\n",
    "        FFTi =np.apply_along_axis(np.argmax,1,( np.apply_along_axis(abs,1,np.apply_along_axis(np.fft.fft,1,Dset_int))))\n",
    "        FFTi = pd.DataFrame(FFTi)\n",
    "        for j in range(Dset_int.shape[0]) : \n",
    "            Col.iloc[j,0] = FFTf.iloc[abs(FFTi.iloc[j,0]),0]\n",
    "\n",
    "        Features[str(i)+'_freq_max_amplitude_fft'] = Col  \n",
    "        \n",
    "    return Features\n",
    "\n",
    "def mean_amplitude_fft(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(np.mean,1,(np.apply_along_axis(abs,1,np.apply_along_axis(np.fft.fft,1,Dset_int))))\n",
    "        Features[str(i)+'_mean_amplitude_fft']=Col\n",
    "       \n",
    "    return Features\n",
    "\n",
    "def peak(Features, lst_data, dset):\n",
    "    \n",
    "    for i in  lst_data:\n",
    "        \n",
    "        Dset_int=dset[i]\n",
    "        Tableau_signal = pd.DataFrame(Dset_int[:,:])\n",
    "        Col = pd.DataFrame(index = np.arange(Dset_int.shape[0]))\n",
    "        Col['peak'] = \"\"\n",
    "        DF = np.apply_along_axis(scipy.signal.find_peaks,1,np.apply_along_axis(abs,1,Dset_int),distance = Dset_int.shape[1])\n",
    "        DF = pd.DataFrame(DF)\n",
    "        DF = DF[0]\n",
    "        DF = DF.apply(int)\n",
    "        \n",
    "        for j in range(Dset_int.shape[0]) : \n",
    "            Col.iloc[j,0] = Tableau_signal.iloc[j,DF.iloc[j]]/np.mean(abs(Tableau_signal.iloc[j,:]))\n",
    "        \n",
    "        Features[str(i)+'_peak'] = Col   \n",
    "        \n",
    "    return Features\n",
    "\n",
    "def puissance_moy_periodogram(Features, lst_data, dset) : \n",
    "   \n",
    "     \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Periodrogram_array =np.apply_along_axis(scipy.signal.periodogram,1,Dset_int)\n",
    "        Periodrogram_array = Periodrogram_array[:,1,:] \n",
    "        Col = np.apply_along_axis(np.mean,1,Periodrogram_array)\n",
    "        Features[str(i)+'__mean_power_periodogram']=Col\n",
    "       \n",
    "    return Features\n",
    " \n",
    "def freq_max_power_periodogram(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Periodrogram_array =np.apply_along_axis(scipy.signal.periodogram,1,Dset_int)\n",
    "        Periodrogram_array = Periodrogram_array[:,1,:] \n",
    "        Col = np.apply_along_axis(max,1,Periodrogram_array)\n",
    "        Features[str(i)+'_freq_max_power_periodogram']=Col\n",
    "       \n",
    "    return Features\n",
    "\n",
    "def freq_max_value_periodogram(Features, lst_data, dset):\n",
    "    \n",
    "    for i in  lst_data:\n",
    "        \n",
    "        Dset_int=dset[i]\n",
    "        Col = pd.DataFrame(index = np.arange(Dset_int.shape[0]))\n",
    "        Col['freq'] = \"\"\n",
    "        Periodrogram_array = np.apply_along_axis(scipy.signal.periodogram,1,Dset_int)\n",
    "        Periodogram_frequences = Periodrogram_array[1,0,:]\n",
    "        Periodogram_frequences = pd.DataFrame(Periodogram_frequences)\n",
    "        Periodrogram_array = Periodrogram_array[:,1,:] \n",
    "        Periodrogram_array = np.apply_along_axis(np.argmax,1,Periodrogram_array)\n",
    "        Periodrogram_array = pd.DataFrame(Periodrogram_array)\n",
    "        \n",
    "        for j in range(Dset_int.shape[0]) : \n",
    "            Col.iloc[j,0] = Periodogram_frequences.iloc[abs(Periodrogram_array.iloc[j,0]),0]\n",
    "    \n",
    "        Features[str(i)+'_freq_max_value_periodrogram'] = Col  \n",
    "        \n",
    "    return Features\n",
    " \n",
    "#Mean_POWER = mean_amplitude_fft(Features, lst_data, dset)\n",
    "\n",
    "#Bin frequences\n",
    "\n",
    "def bin_power(X, Band, Fs):\n",
    "    \n",
    "    \"\"\"Compute power in each frequency bin specified by Band from FFT result of\n",
    "    X. By default, X is a real signal.\n",
    "    Note\n",
    "    -----\n",
    "    A real signal can be synthesized, thus not real.\n",
    "    Parameters\n",
    "    -----------\n",
    "    Band\n",
    "        list\n",
    "        boundary frequencies (in Hz) of bins. They can be unequal bins, e.g.\n",
    "        [0.5,4,7,12,30] which are delta, theta, alpha and beta respectively.\n",
    "        You can also use range() function of Python to generate equal bins and\n",
    "        pass the generated list to this function.\n",
    "        Each element of Band is a physical frequency and shall not exceed the\n",
    "        Nyquist frequency, i.e., half of sampling frequency.\n",
    "     X\n",
    "        list\n",
    "        a 1-D real time series.\n",
    "    Fs\n",
    "        integer\n",
    "        the sampling rate in physical frequency\n",
    "    Returns\n",
    "    -------\n",
    "    Power\n",
    "        list\n",
    "        spectral power in each frequency bin.\n",
    "    Power_ratio\n",
    "        list\n",
    "        spectral power in each frequency bin normalized by total power in ALL\n",
    "        frequency bins.\n",
    "    \"\"\"\n",
    "\n",
    "    C = np.fft.fft(X)\n",
    "    C = abs(C)\n",
    "    Power = np.zeros(len(Band) - 1)\n",
    "    for Freq_Index in range(0, len(Band) - 1):\n",
    "        Freq = float(Band[Freq_Index])\n",
    "        Next_Freq = float(Band[Freq_Index + 1])\n",
    "        Power[Freq_Index] = np.sum(\n",
    "            C[int(Freq / Fs * len(X)) : int(Next_Freq / Fs * len(X))]\n",
    "        )\n",
    "    Power_Ratio = Power / sum(Power)\n",
    "    return Power, Power_Ratio\n",
    "\n",
    "\n",
    "def bin_power_features(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Resultat_int = np.apply_along_axis(bin_power,1,Dset_int, Band = [0.5,4,7,12,30], Fs = len(Dset_int[1])/30)\n",
    "        Array_somme_frequence = Resultat_int[:,0,:]\n",
    "        Array_somme_frequence = pd.DataFrame(Array_somme_frequence)\n",
    "        Array_somme_frequence.columns = ['Delta','Theta','Alpha','Beta']\n",
    "        Array_somme_frequence_normalisee = Resultat_int[:,1,:]\n",
    "        Array_somme_frequence_normalisee = pd.DataFrame(Array_somme_frequence_normalisee)\n",
    "        Array_somme_frequence_normalisee.columns = ['Delta_N','Theta_N','Alpha_N','Beta_N']\n",
    "        Features[str(i)+'Delta'] = Array_somme_frequence['Delta']\n",
    "        Features[str(i)+'Theta'] = Array_somme_frequence['Theta']\n",
    "        Features[str(i)+'Alpha'] = Array_somme_frequence['Alpha']\n",
    "        Features[str(i)+'Beta'] = Array_somme_frequence['Beta']\n",
    "        Features[str(i)+'Delta_N'] = Array_somme_frequence_normalisee['Delta_N']\n",
    "        Features[str(i)+'Theta_N'] = Array_somme_frequence_normalisee['Theta_N']\n",
    "        Features[str(i)+'Alpha_N'] = Array_somme_frequence_normalisee['Alpha_N']\n",
    "        Features[str(i)+'Beta_N'] = Array_somme_frequence_normalisee['Beta_N']\n",
    "       \n",
    "    return Features\n",
    "\n",
    "def max_value_sampled(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        length_sample = len(Dset_int[0])\n",
    "        number_of_samples = int(length_sample/100)\n",
    "        Data_Frame = pd.DataFrame()\n",
    "        \n",
    "        for j in range(number_of_samples):\n",
    "            Sample = Dset_int[:,j*100:(j+1)*100]\n",
    "            Col_M = np.apply_along_axis(max,1,Sample)\n",
    "            Col_m = np.apply_along_axis(min,1,Sample)\n",
    "            Col = Col_M-Col_m\n",
    "            Features[str(i)+'_max_value_sample_'+str(j)] = Col\n",
    "            Data_Frame['_max_value_sample_'+str(j)] = Col\n",
    "        \n",
    "        Features[str(i)+'MMD'+str(j)] = Data_Frame.sum(1)\n",
    "    \n",
    "    return Features\n",
    "\n",
    "def ESIS(X, Band, Fs):\n",
    "\n",
    "    C = np.fft.fft(X)\n",
    "    C = abs(C)\n",
    "    C = np.square(C)\n",
    "    Power = np.zeros(len(Band) - 1)\n",
    "    for Freq_Index in range(0, len(Band) - 1):\n",
    "        Freq = float(Band[Freq_Index])\n",
    "        Next_Freq = float(Band[Freq_Index + 1])\n",
    "        Power[Freq_Index] = np.sum(\n",
    "            C[int(Freq / Fs * len(X)) : int(Next_Freq / Fs * len(X))]\n",
    "        )\n",
    "        Power[Freq_Index] = 100*Power[Freq_Index]*1/2*(Next_Freq+Freq)\n",
    "    if sum(Power) == 0 :\n",
    "        Power_Ratio = np.zeros(len(Band) - 1)\n",
    "    else :\n",
    "        Power_Ratio = Power / sum(Power)\n",
    "    \n",
    "    return Power, Power_Ratio\n",
    "\n",
    "def ESIS_features(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Resultat_int = np.apply_along_axis(ESIS,1,Dset_int, Band = [0.5,4,7,12,30], Fs = len(Dset_int[1])/30)\n",
    "        Array_somme_frequence = Resultat_int[:,0,:]\n",
    "        Array_somme_frequence = pd.DataFrame(Array_somme_frequence)\n",
    "        Array_somme_frequence.columns = ['Delta_esis','Theta_esis','Alpha_esis','Beta_esis']\n",
    "        Array_somme_frequence_normalisee = Resultat_int[:,1,:]\n",
    "        Array_somme_frequence_normalisee = pd.DataFrame(Array_somme_frequence_normalisee)\n",
    "        Array_somme_frequence_normalisee.columns = ['Delta_esis__N','Theta_esis__N','Alpha_esis__N','Beta_esis__N']\n",
    "        Features[str(i)+'Delta_esis'] = Array_somme_frequence['Delta_esis']\n",
    "        Features[str(i)+'Theta_esis'] = Array_somme_frequence['Theta_esis']\n",
    "        Features[str(i)+'Alpha_esis'] = Array_somme_frequence['Alpha_esis']\n",
    "        Features[str(i)+'Beta_esis'] = Array_somme_frequence['Beta_esis']\n",
    "        Features[str(i)+'Delta_esis__N'] = Array_somme_frequence_normalisee['Delta_esis__N']\n",
    "        Features[str(i)+'Theta_esis__N'] = Array_somme_frequence_normalisee['Theta_esis__N']\n",
    "        Features[str(i)+'Alpha_esis__N'] = Array_somme_frequence_normalisee['Alpha_esis__N']\n",
    "        Features[str(i)+'Beta_esis__N'] = Array_somme_frequence_normalisee['Beta_esis__N']\n",
    "       \n",
    "    return Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fish_info_feat(Features, lst_data, dset):\n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col=np.apply_along_axis(lambda x : pyeeg.fisher_info(x,1,4),1, Dset_int)\n",
    "        Features[str(i)+'_fish_info']=Col\n",
    "    return Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extraction on a Dataset\n",
    "\n",
    "Function qui prend un H5 en entrée, et qui lui applique toutes nos fonctions d'extraction de Features. Il retourne un DataFrame contenant tous les Features extraits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features=pd.DataFrame()\n",
    "all_dset=list(f.keys())\n",
    "col_eeg_oxy=['eeg_1', 'eeg_2', 'eeg_3', 'eeg_4', 'eeg_5', 'eeg_6', 'eeg_7', 'pulse_oximeter_infrared']\n",
    "col_acc=['accelerometer_x', 'accelerometer_y', 'accelerometer_z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features=abs_mean(Features,all_dset,dset)\n",
    "Features=max_value(Features,all_dset,dset)\n",
    "Features=min_value(Features,all_dset,dset)\n",
    "Features=max_abs_value(Features,all_dset,dset)\n",
    "Features=abs_mean_derivate(Features,all_dset,dset)\n",
    "Features=max_abs_derivate(Features,all_dset,dset)\n",
    "Features=max_value_derivate(Features,all_dset,dset)\n",
    "Features=min_value_derivate(Features,all_dset,dset)\n",
    "Features=freq_max_power(Features,all_dset,dset)\n",
    "Features=freq_max_value(Features,all_dset,dset)\n",
    "Features=peak(Features,all_dset,dset)\n",
    "Features=fish_info_feat(Features,col_eeg_oxy,dset)\n",
    "Features= bin_power_features(Features, lst_data, dset)\n",
    "Features=freq_max_power_periodogram(Features, lst_data, dset)\n",
    "Features=puissance_moy_periodogram(Features, lst_data, dset)\n",
    "Features=max_amplitude_fft(Features, lst_data, dset)\n",
    "Features=freq_max_amplitude_fft(Features, lst_data, dset)\n",
    "Features=mean_amplitude_fft(Features, lst_data, dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Testing\n",
    "\n",
    "Ensemble de cellules permettant d'entrainer le modèle sur une partie du Dataset pour le tester sur une autre .\n",
    "\n",
    "Afin de créer le Dataset, il y a deux possibilités : lire un csv ou recréer la Base de données\n",
    "\n",
    "Voir cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_train = pd.read_csv('Features_train_3.csv')\n",
    "Features_train=Features_train.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('train_y.csv').values[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Features_train.values, y, test_size = 0.25, random_state = 0)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Le Random Forest consiste en la création de plusieurs arbres de décision. Pour chaque évalutation, le résultat obtenu est le résultat obtenu par la majorité des arbres (ou une moyenne dans le cas d'une regression).\n",
    "Chaque arbre de décision peut être représenté par un ensemble de feuilles et noeuds. A chaque noeud, une condition portant sur les valeurs de notre échantillon enverra dans un des noeuds suivants et ainsi de suite.\n",
    "\n",
    "Lors de la phase d'entraînement, le modèle essaye de partitionner au mieux les données. Cela consiste en partitionner les données de façon à améliorer le critère de gini (plusieurs mesures possibles). L'arbre continue à créer des noeuds jusqu'à ce que le partitionnement de la donnée n'améliore plus le résultat ou qu'une seule classe soit atteinte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multilabel-indicator is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-2070999be2ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_m\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: multilabel-indicator is not supported"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    " \n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6083777290295623"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test,y_test_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG BOOST\n",
    "\n",
    "Cet algorithme (Extreme Gradient Boosting) est en fait un ensemble d'arbres de décision (Random Forest) couplé à un Booster de Gradient. Leurs structures sont très similaires. La seule véritable différence réside dans la façon avec laquelle le modèle est entraîné. Lorsqu'un nouvel arbre est créé, il vise à réduire les erreurs faites par les arbres précédents. Les arbres ont également moins de séparations que pour un Arbre décisionnel traditionnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 593,   22,  176,   14,   95],\n",
       "       [  69,   20,  141,    8,   76],\n",
       "       [ 127,   37, 3506,  191,  454],\n",
       "       [  30,   11,  325, 1048,   26],\n",
       "       [  77,   28,  629,   29, 1841]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "classifier = XGBClassifier(max_depth=50, n_estimators=10, n_jobs=-1)\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7320589157004074"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double split\n",
    "\n",
    "Pour ce modèle, nous avons séparé les données en deux parties : categories 2 et 4/ le reste.\n",
    "Cette idée est née de la proximité des données pour les catégories 2 et 4.\n",
    "Nous avons donc séparé les données grâce à un premier RandomForest. Puis nous avons classifié chaque partie séparemment grâce à deux nouveaux RandomForest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_cat(y_cat) : \n",
    "    y = y_cat\n",
    "    y[y == 1] = 0\n",
    "    y[y == 3] = 0\n",
    "    y[y == 2] = 1\n",
    "    y[y == 4] = 1\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_doub = bin_cat(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1750,  904],\n",
       "       [ 376, 6543]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Features_train.values, y_doub, test_size = 0.25, random_state = 0)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8662906090044918"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = model.predict(Features_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_train['y_pred_1'] = y_pred_1\n",
    "Features_train['y'] = y\n",
    "Train_1 = Features_train.loc[Features_train['y_pred_1']==1,:]\n",
    "Train_0 = Features_train.loc[Features_train['y_pred_1']==0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 59,  50],\n",
       "       [ 28, 156]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RD Forest train0\n",
    "\n",
    "X_train_0, X_test_0, y_train_0, y_test_0 = train_test_split(Train_0.iloc[:,:-2].values,Train_0.iloc[:,-1], test_size = 0.25, random_state = 0)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train_0=sc.fit_transform(X_train_0)\n",
    "X_test_0=sc.transform(X_test_0)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_0 = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model_0.fit(X_train_0,y_train_0)\n",
    "y_pred_0 = model.predict(X_test_0)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_0 = confusion_matrix(y_test_0, y_pred_0)\n",
    "cm_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7337883959044369"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test_0,y_test_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1738,  875],\n",
       "       [ 613, 6054]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RD Forest train0\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(Train_1.iloc[:,:-2].values,Train_1.iloc[:,-1], test_size = 0.25, random_state = 0)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train_1=sc.fit_transform(X_train_1)\n",
    "X_test_1=sc.transform(X_test_1)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_1 = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model_1.fit(X_train_1,y_train_1)\n",
    "y_pred_1 = model.predict(X_test_1)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_1 = confusion_matrix(y_test_1, y_pred_1)\n",
    "cm_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8671336206896552"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.score(X_test_1,y_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 4106,  6597],\n",
       "       [ 8631, 18955]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_0['y_pred_f'] = model_0.predict(Train_0.iloc[:,:-2].values)\n",
    "Train_1['y_pred_f'] = model_1.predict(Train_1.iloc[:,:-2].values)\n",
    "\n",
    "Res = pd.concat([Train_0,Train_1])\n",
    "cm_f = confusion_matrix(Res['y'],Res['y_pred_f'])\n",
    "cm_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de tester les résultats de notre modèle, nous avons utilisé la fonction cross_val_score. Cela permet de découper notre Training set en 10 parties égales, d'entraîner le modèle sur 9 parties et de le tester sur le dernière. Cela nous permet d'avoir une valeur moyenne (ainsi que la variance) du modèle. Cela permet de facilement souligner un possible overfitting, tout en ayant une bonne estimation statistique de notre modèle.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "Feat_tot=Features_train\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_kfold = StandardScaler()\n",
    "Feat_tot = sc_kfold.fit_transform(Feat_tot)\n",
    "\n",
    "## Model \n",
    "classifier = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "\n",
    "scores = cross_val_score(classifier, Feat_tot, y, cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest + one Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.62 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(categorical_features=[0])\n",
    "y_mult = ohe.fit_transform(y.reshape((-1,1))).toarray()\n",
    "\n",
    "Feat_tot=Features_train\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_kfold = StandardScaler()\n",
    "Feat_tot = sc_kfold.fit_transform(Feat_tot)\n",
    "\n",
    "## Model \n",
    "classifier = RandomForestClassifier(n_estimators=100, n_jobs=-1,)\n",
    "\n",
    "scores = cross_val_score(classifier, Feat_tot, y_mult, cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "Feat_tot=Features_train\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_kfold = StandardScaler()\n",
    "Feat_tot = sc_kfold.fit_transform(Feat_tot)\n",
    "\n",
    "## Model \n",
    "classifier = XGBClassifier(max_depth=50, n_estimators=100, n_jobs=-1)\n",
    "\n",
    "scores = cross_val_score(classifier, Feat_tot, y, cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training sans test\n",
    "\n",
    "Ensemble de cellules permettant d'entrainer le modèle sur  Dataset pour le tester sur une autre \n",
    "\n",
    "Voir cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_pred = h5py.File('test.h5', 'r')\n",
    "y_train=pd.read_csv('train_y.csv').values[:,1]\n",
    "dset = h5py.File('train.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_train = pd.read_csv('Features_train_5.csv')\n",
    "Features_train = Features_train.iloc[:,2:]\n",
    "Features_test = pd.read_csv('Features_test_5.csv')\n",
    "Features_test = Features_test.iloc[:,2:]\n",
    "y_train = pd.read_csv('train_y.csv').values[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = Features_train.loc[:,:].values\n",
    "X_pred = Features_test.loc[:,:].values\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_pred = sc.transform(X_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=300, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model training\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=300,n_jobs=-1)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_pred)\n",
    "Res=pd.DataFrame()\n",
    "Res['sleep_stage']=y_pred\n",
    "Res.index.name='id'\n",
    "Res.to_csv('y_pred.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=150, min_child_weight=1, missing=None, n_estimators=300,\n",
       "       n_jobs=-1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "classifier_xg = XGBClassifier(max_depth=150, n_estimators=300, n_jobs=-1)\n",
    "classifier_xg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier_xg.predict(X_pred)\n",
    "Res=pd.DataFrame()\n",
    "Res['sleep_stage']=y_pred\n",
    "Res.index.name='id'\n",
    "Res.to_csv('y_pred_xg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "frequency=500\n",
    "duration =5000\n",
    "winsound.Beep(frequency,duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Random_forrest_Dreem.joblib']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(model, 'Random_forest_Dreem.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Threshold ML\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_train = pd.read_csv('Features_train_2.csv') \n",
    "Features_train = Features_train.iloc[:,1:]\n",
    "Features_test = pd.read_csv('Features_test_2.csv')\n",
    "Features_test = Features_test.iloc[:,1:]\n",
    "y_train = pd.read_csv('train_y.csv').iloc[:,[1]] #DF (WITH index)\n",
    "y_train['y_cat'] = bin_cat(y_train.values.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separation des Sleep Stages 0,1,3 de 2,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = Features_train.loc[:,:].values\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_bin = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model_bin.fit(X_train,y_train.iloc[:,1].values)\n",
    "\n",
    "\n",
    "#y_train['y_pred_ts'] = y_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extraction des Sleep stages 2,4 and deuxième prédiction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1 = Features_train.loc[y_train['y_cat']==1,:]\n",
    "y_train_1 = y_train.loc[y_train['y_cat']==1,'sleep_stage'].values\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_2=StandardScaler()\n",
    "X_train_1=sc_2.fit_transform(X_train_1)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_2_4 = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model_2_4 .fit(X_train_1,y_train_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest sur le Dataset entier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2 = Features_train.loc[:,:].values\n",
    "y_train_2 = y_train.iloc[:,0].values\n",
    "X_train_2 = sc.transform(X_train_2)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_glob = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model_glob.fit(X_train_2,y_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction du test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "X_test = Features_test.loc[:,:].values\n",
    "X_test = sc.transform(X_test)\n",
    "y_test = pd.DataFrame()\n",
    "y_test['y_glob'] = model_glob.predict(X_test)\n",
    "y_test['y_cat'] = model_bin.predict(X_test) \n",
    "\n",
    "\n",
    "\n",
    "X_test_1 = Features_test.loc[y_test['y_cat']==1,:]\n",
    "X_test_1=sc_2.transform(X_test_1)\n",
    "y_int = y_test.loc[y_test['y_cat']==1,['y_cat']]\n",
    "y_int['y_bin'] = model_2_4.predict(X_test_1)\n",
    "\n",
    "\n",
    "\n",
    "y_test = y_test.merge(y_int.loc[:,['y_bin']],right_index = True, left_index = True, how='outer')\n",
    "\n",
    "\n",
    "y_test['sleep_stage'] = y_test['y_bin'].fillna(y_test['y_glob'])\n",
    "\n",
    "y_test.index.name='id'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test['sleep_stage'] = y_test['sleep_stage'].astype('int64')\n",
    "y_test.loc[:,['sleep_stage']].to_csv('y_pred_triple.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Présentation des résultats\n",
    "\n",
    "Voici la partie finale permettant de comparer les résultats obtenus.\n",
    "On s'apercoit qu'en augmentant la compléxité du modèle (profondeur et nombre de features), on n'augmente pas toujours notre score. En effet, si le meilleur score est atteint pour 300 arbres et 200 features, on peut approcher ce résultat avec 200 arbres et 50 features (ceux présentés dans la thèse)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "plt.scatter([0,1,2,3,4,5,6],[0.609,0.62444,0.63112,0.62604,0.63224,0.63171,0.63069], label = 'XG BOOST', c='r')\n",
    "plt.scatter([0,2 ], [0.62102,0.62599] ,c='b', label = 'Random Forest')\n",
    "plt.plot([0,6],[0.63,0.63],'k', label = 'XGBoost score')\n",
    "plt.plot([0,6],[0.623,0.623], label = 'XGBoost score')\n",
    "plt.title('Evolution de la précision des modèles en fonction de la complexité du modèle')\n",
    "plt.xlabel('Compléxité du modèle', fontsize=14, color='black')\n",
    "plt.ylabel('F1 score', fontsize=14, color='black')\n",
    "plt.legend()\n",
    "plt.show();\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
