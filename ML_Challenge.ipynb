{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pyeeg\n",
    "import scipy.signal\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importer les données\n",
    "Les données sont sous la forme h5. Il faut donc les importer puis créer x dataset en fonction du nombre de dataset présents dans le fichier (10). On utilise ici r+ pour avoir droit de lecture et d'écriture.\n",
    "\n",
    "Chaque Dataset représente les mesures des accéléromètres, eeg et l'oximeter pour 38289 différentes mesures.\n",
    "Les catégories de sommeil sont des entiers entre 0 et 5 présentés dans un csv."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = h5py.File('train.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = pd.read_csv('train_y.csv').values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Liste des données et extraction \n",
    "\n",
    "Voici la liste des jeux de données disponible et leur extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['accelerometer_x', 'accelerometer_y', 'accelerometer_z', 'eeg_1', 'eeg_2', 'eeg_3', 'eeg_4', 'eeg_5', 'eeg_6', 'eeg_7', 'pulse_oximeter_infrared']\n"
     ]
    }
   ],
   "source": [
    "print(list(f.keys()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dsetx = f['accelerometer_x']\n",
    "dsety = f['accelerometer_y']\n",
    "dsetz = f['accelerometer_z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dsetx4 = dsetx[y_test[:,1]==4,:]\n",
    "dsety4 = dsety[y_test[:,1]==4,:]\n",
    "dsetz4 = dsetz[y_test[:,1]==4,:]\n",
    "\n",
    "name=1\n",
    "for i in [dsetx4,dsety4,dsetz4] :\n",
    "    plt.scatter([i for i in range (len(i[1,:]))],i[1,:])\n",
    "    plt.savefig('acc_'+str(name)+'.png')\n",
    "    plt.clf()\n",
    "    name+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotting(h5,list_data_name, output):\n",
    "    for dset_name in list_data_name :\n",
    "        dset=h5[dset_name]\n",
    "        dset2=dset[y_test[:,1]==output,:]\n",
    "        plt.scatter([i for i in range (len(dset2[1,:]))],dset2[1,:])\n",
    "        plt.savefig('output_'+str(output)+dset_name+'.png')\n",
    "        plt.clf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for j in range(1,5):\n",
    "    plotting(f,['accelerometer_x', 'accelerometer_y', 'accelerometer_z'],j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Features extraction\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parler de l'extraction de Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abs_mean(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(np.mean,1,np.apply_along_axis(abs,1,Dset_int))\n",
    "        Features[str(i)+'_abs_mean']=Col\n",
    "        \n",
    "        \n",
    "    return Features\n",
    "\n",
    "def mean(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(np.mean,1,Dset_int)\n",
    "        Features[str(i)+'_mean']=Col\n",
    "    return Features\n",
    "\n",
    "def max_value(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(max,1,Dset_int)\n",
    "        Features[str(i)+'_max_value']=Col\n",
    "        \n",
    "    return Features\n",
    "\n",
    "\n",
    "def min_value(Features, lst_data, dset) :        \n",
    "    \n",
    "    for i in lst_data :\n",
    "        \n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(min,1,Dset_int)\n",
    "        Features[str(i)+'_min_value']=Col\n",
    "        \n",
    "    return Features\n",
    "\n",
    "\n",
    "def max_abs_value(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(max,1,np.apply_along_axis(abs,1,Dset_int))\n",
    "        Features[str(i)+'_max_abs_value']=Col\n",
    "        \n",
    "    return Features\n",
    "\n",
    "\n",
    "def abs_mean_derivate(Features, lst_data, dset):\n",
    "   \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        multiplicative_coef = len(Dset_int[1])/30\n",
    "        Col = np.apply_along_axis(np.mean,1,np.apply_along_axis(abs,1,np.apply_along_axis(np.gradient,1,Dset_int)*multiplicative_coef))\n",
    "        Features[str(i)+'_abs_mean_derivate']=Col\n",
    "        \n",
    "    return Features\n",
    "\n",
    "def max_abs_derivate(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        multiplicative_coef = len(Dset_int[1])/30\n",
    "        Col = np.apply_along_axis(max,1,np.apply_along_axis(abs,1,np.apply_along_axis(np.gradient,1,Dset_int)*multiplicative_coef))\n",
    "        Features[str(i)+'_max_abs_derivate']=Col\n",
    "        \n",
    "    return Features\n",
    "\n",
    "\n",
    "def max_value_derivate(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        multiplicative_coef = len(Dset_int[1])/30\n",
    "        Col = np.apply_along_axis(max,1,np.apply_along_axis(np.gradient,1,Dset_int)*multiplicative_coef)\n",
    "        Features[str(i)+'_max_value_derivate']=Col\n",
    "       \n",
    "    return Features\n",
    "\n",
    "\n",
    "def min_value_derivate(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        multiplicative_coef = len(Dset_int[1])/30\n",
    "        Col = np.apply_along_axis(min,1,np.apply_along_axis(np.gradient,1,Dset_int)*multiplicative_coef)\n",
    "        Features[str(i)+'_min_value_derivate']=Col\n",
    "        \n",
    "    return Features\n",
    "\n",
    "\n",
    "def freq_max_power(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(max,1,(np.apply_along_axis(abs,1,np.apply_along_axis(np.fft.fft,1,Dset_int))))\n",
    "        Features[str(i)+'_freq_max_power']=Col\n",
    "       \n",
    "    return Features\n",
    "\n",
    "\n",
    "def freq_max_value(Features, lst_data, dset):\n",
    "    \n",
    "    \n",
    "    for i in  lst_data:\n",
    "        Dset_int=dset[i]\n",
    "        Col = pd.DataFrame(index = np.arange(Dset_int.shape[0]))\n",
    "        Col['freq'] = \"\"\n",
    "        FFTf = np.fft.fftfreq(len(Dset_int[1]))*len(Dset_int[1])/30\n",
    "        FFTf = pd.DataFrame(FFTf)\n",
    "        FFTi =np.apply_along_axis(np.argmax,1,( np.apply_along_axis(abs,1,np.apply_along_axis(np.fft.fft,1,Dset_int))))\n",
    "        FFTi = pd.DataFrame(FFTi)\n",
    "        for j in range(Dset_int.shape[0]) : \n",
    "            Col.iloc[j,0] = FFTf.iloc[FFTi.iloc[j,0],0]\n",
    "\n",
    "        Features[str(i)+'_freq_max_value'] = Col  \n",
    "        \n",
    "    return Features\n",
    "\n",
    "\n",
    "def max_amplitude_fft(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(max,1,(np.apply_along_axis(abs,1,np.apply_along_axis(np.fft.fft,1,Dset_int))))\n",
    "        Features[str(i)+'_max_amplitude_fft']=Col\n",
    "       \n",
    "    return Features\n",
    "\n",
    "#MAX_POWER = max_amplitude_fft(Features, lst_data, dset)\n",
    "\n",
    "def freq_max_amplitude_fft(Features, lst_data, dset):\n",
    "    \n",
    "    \n",
    "    for i in  lst_data:\n",
    "        Dset_int=dset[i]\n",
    "        Col = pd.DataFrame(index = np.arange(Dset_int.shape[0]))\n",
    "        Col['freq'] = \"\"\n",
    "        FFTf = np.fft.fftfreq(len(Dset_int[1]))*len(Dset_int[1])/30\n",
    "        FFTf = pd.DataFrame(FFTf)\n",
    "        FFTi =np.apply_along_axis(np.argmax,1,( np.apply_along_axis(abs,1,np.apply_along_axis(np.fft.fft,1,Dset_int))))\n",
    "        FFTi = pd.DataFrame(FFTi)\n",
    "        for j in range(Dset_int.shape[0]) : \n",
    "            Col.iloc[j,0] = FFTf.iloc[abs(FFTi.iloc[j,0]),0]\n",
    "\n",
    "        Features[str(i)+'_freq_max_amplitude_fft'] = Col  \n",
    "        \n",
    "    return Features\n",
    "\n",
    "def mean_amplitude_fft(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col = np.apply_along_axis(np.mean,1,(np.apply_along_axis(abs,1,np.apply_along_axis(np.fft.fft,1,Dset_int))))\n",
    "        Features[str(i)+'_mean_amplitude_fft']=Col\n",
    "       \n",
    "    return Features\n",
    "\n",
    "def peak(Features, lst_data, dset):\n",
    "    \n",
    "    for i in  lst_data:\n",
    "        \n",
    "        Dset_int=dset[i]\n",
    "        Tableau_signal = pd.DataFrame(Dset_int[:,:])\n",
    "        Col = pd.DataFrame(index = np.arange(Dset_int.shape[0]))\n",
    "        Col['peak'] = \"\"\n",
    "        DF = np.apply_along_axis(scipy.signal.find_peaks,1,np.apply_along_axis(abs,1,Dset_int),distance = Dset_int.shape[1])\n",
    "        DF = pd.DataFrame(DF)\n",
    "        DF = DF[0]\n",
    "        DF = DF.apply(int)\n",
    "        \n",
    "        for j in range(Dset_int.shape[0]) : \n",
    "            Col.iloc[j,0] = Tableau_signal.iloc[j,DF.iloc[j]]/np.mean(abs(Tableau_signal.iloc[j,:]))\n",
    "        \n",
    "        Features[str(i)+'_peak'] = Col   \n",
    "        \n",
    "    return Features\n",
    "\n",
    "def puissance_moy_periodogram(Features, lst_data, dset) : \n",
    "   \n",
    "     \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Periodrogram_array =np.apply_along_axis(scipy.signal.periodogram,1,Dset_int)\n",
    "        Periodrogram_array = Periodrogram_array[:,1,:] \n",
    "        Col = np.apply_along_axis(np.mean,1,Periodrogram_array)\n",
    "        Features[str(i)+'__mean_power_periodogram']=Col\n",
    "       \n",
    "    return Features\n",
    " \n",
    "def freq_max_power_periodogram(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Periodrogram_array =np.apply_along_axis(scipy.signal.periodogram,1,Dset_int)\n",
    "        Periodrogram_array = Periodrogram_array[:,1,:] \n",
    "        Col = np.apply_along_axis(max,1,Periodrogram_array)\n",
    "        Features[str(i)+'_freq_max_power_periodogram']=Col\n",
    "       \n",
    "    return Features\n",
    "\n",
    "def freq_max_value_periodogram(Features, lst_data, dset):\n",
    "    \n",
    "    for i in  lst_data:\n",
    "        \n",
    "        Dset_int=dset[i]\n",
    "        Col = pd.DataFrame(index = np.arange(Dset_int.shape[0]))\n",
    "        Col['freq'] = \"\"\n",
    "        Periodrogram_array = np.apply_along_axis(scipy.signal.periodogram,1,Dset_int)\n",
    "        Periodogram_frequences = Periodrogram_array[1,0,:]\n",
    "        Periodogram_frequences = pd.DataFrame(Periodogram_frequences)\n",
    "        Periodrogram_array = Periodrogram_array[:,1,:] \n",
    "        Periodrogram_array = np.apply_along_axis(np.argmax,1,Periodrogram_array)\n",
    "        Periodrogram_array = pd.DataFrame(Periodrogram_array)\n",
    "        \n",
    "        for j in range(Dset_int.shape[0]) : \n",
    "            Col.iloc[j,0] = Periodogram_frequences.iloc[abs(Periodrogram_array.iloc[j,0]),0]\n",
    "    \n",
    "        Features[str(i)+'_freq_max_value_periodrogram'] = Col  \n",
    "        \n",
    "    return Features\n",
    " \n",
    "#Mean_POWER = mean_amplitude_fft(Features, lst_data, dset)\n",
    "\n",
    "#Bin frequences\n",
    "\n",
    "def bin_power(X, Band, Fs):\n",
    "    \n",
    "    \"\"\"Compute power in each frequency bin specified by Band from FFT result of\n",
    "    X. By default, X is a real signal.\n",
    "    Note\n",
    "    -----\n",
    "    A real signal can be synthesized, thus not real.\n",
    "    Parameters\n",
    "    -----------\n",
    "    Band\n",
    "        list\n",
    "        boundary frequencies (in Hz) of bins. They can be unequal bins, e.g.\n",
    "        [0.5,4,7,12,30] which are delta, theta, alpha and beta respectively.\n",
    "        You can also use range() function of Python to generate equal bins and\n",
    "        pass the generated list to this function.\n",
    "        Each element of Band is a physical frequency and shall not exceed the\n",
    "        Nyquist frequency, i.e., half of sampling frequency.\n",
    "     X\n",
    "        list\n",
    "        a 1-D real time series.\n",
    "    Fs\n",
    "        integer\n",
    "        the sampling rate in physical frequency\n",
    "    Returns\n",
    "    -------\n",
    "    Power\n",
    "        list\n",
    "        spectral power in each frequency bin.\n",
    "    Power_ratio\n",
    "        list\n",
    "        spectral power in each frequency bin normalized by total power in ALL\n",
    "        frequency bins.\n",
    "    \"\"\"\n",
    "\n",
    "    C = np.fft.fft(X)\n",
    "    C = abs(C)\n",
    "    Power = np.zeros(len(Band) - 1)\n",
    "    for Freq_Index in range(0, len(Band) - 1):\n",
    "        Freq = float(Band[Freq_Index])\n",
    "        Next_Freq = float(Band[Freq_Index + 1])\n",
    "        Power[Freq_Index] = np.sum(\n",
    "            C[int(Freq / Fs * len(X)) : int(Next_Freq / Fs * len(X))]\n",
    "        )\n",
    "    Power_Ratio = Power / sum(Power)\n",
    "    return Power, Power_Ratio\n",
    "\n",
    "\n",
    "def bin_power_features(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Resultat_int = np.apply_along_axis(bin_power,1,Dset_int, Band = [0.5,4,7,12,30], Fs = len(Dset_int[1])/30)\n",
    "        Array_somme_frequence = Resultat_int[:,0,:]\n",
    "        Array_somme_frequence = pd.DataFrame(Array_somme_frequence)\n",
    "        Array_somme_frequence.columns = ['Delta','Theta','Alpha','Beta']\n",
    "        Array_somme_frequence_normalisee = Resultat_int[:,1,:]\n",
    "        Array_somme_frequence_normalisee = pd.DataFrame(Array_somme_frequence_normalisee)\n",
    "        Array_somme_frequence_normalisee.columns = ['Delta_N','Theta_N','Alpha_N','Beta_N']\n",
    "        Features[str(i)+'Delta'] = Array_somme_frequence['Delta']\n",
    "        Features[str(i)+'Theta'] = Array_somme_frequence['Theta']\n",
    "        Features[str(i)+'Alpha'] = Array_somme_frequence['Alpha']\n",
    "        Features[str(i)+'Beta'] = Array_somme_frequence['Beta']\n",
    "        Features[str(i)+'Delta_N'] = Array_somme_frequence_normalisee['Delta_N']\n",
    "        Features[str(i)+'Theta_N'] = Array_somme_frequence_normalisee['Theta_N']\n",
    "        Features[str(i)+'Alpha_N'] = Array_somme_frequence_normalisee['Alpha_N']\n",
    "        Features[str(i)+'Beta_N'] = Array_somme_frequence_normalisee['Beta_N']\n",
    "       \n",
    "    return Features\n",
    "\n",
    "def max_value_sampled(Features, lst_data, dset):\n",
    "    \n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        length_sample = len(Dset_int[0])\n",
    "        number_of_samples = int(length_sample/100)\n",
    "        Data_Frame = pd.DataFrame()\n",
    "        \n",
    "        for j in range(number_of_samples):\n",
    "            Sample = Dset_int[:,j*100:(j+1)*100]\n",
    "            Col_M = np.apply_along_axis(max,1,Sample)\n",
    "            Col_m = np.apply_along_axis(min,1,Sample)\n",
    "            Col = Col_M-Col_m\n",
    "            Features[str(i)+'_max_value_sample_'+str(j)] = Col\n",
    "            Data_Frame['_max_value_sample_'+str(j)] = Col\n",
    "        \n",
    "        Features[str(i)+'MMD'+str(j)] = Data_Frame.sum(1)\n",
    "    \n",
    "    return Features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset = h5py.File('train.h5', 'r')\n",
    "Features_train = pd.read_csv('Features_train_2.csv')\n",
    "Features_train = max_value_sampled(Features_train, list(dset.keys()), dset)\n",
    "Features_train.to_csv('Features_train_4.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_test = h5py.File('test.h5', 'r')\n",
    "Features_test = pd.read_csv('Features_test_2.csv')\n",
    "Features_train = max_value_sampled(Features_test, list(dset.keys()), dset_test)\n",
    "Features_test.to_csv('Features_test_4.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Signal analysis\n",
    "Pyeeg module \n",
    "Goal : créer des fonctions pour extraire pour un dataset un certain features. \n",
    "On pourra ainsi itérer pour plusieurs fonctions afin d'obtenir nos features pour chaque input.\n",
    "On stocke les valeurs dans un Dataset (Features) afin de garder une tracabilité des colonnes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fish_info_feat(Features, lst_data, dset):\n",
    "    for i in lst_data :\n",
    "        Dset_int=dset[i]\n",
    "        Col=np.apply_along_axis(lambda x : pyeeg.fisher_info(x,1,4),1, Dset_int)\n",
    "        Features[str(i)+'_fish_info']=Col\n",
    "    return Features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features=fish_info_feat(Features, ['eeg_1','eeg_2'],f)\n",
    "Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features extraction on a Dataset\n",
    "\n",
    "Function qui prend un H5 en entrée, et qui lui applique toutes nos fonctions d'extraction de Features. Il retourne un DataFrame contenant tous les Features extraits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features=pd.DataFrame()\n",
    "all_dset=list(f.keys())\n",
    "col_eeg_oxy=['eeg_1', 'eeg_2', 'eeg_3', 'eeg_4', 'eeg_5', 'eeg_6', 'eeg_7', 'pulse_oximeter_infrared']\n",
    "col_acc=['accelerometer_x', 'accelerometer_y', 'accelerometer_z']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features=abs_mean(Features,all_dset,dset)\n",
    "Features=max_value(Features,all_dset,dset)\n",
    "Features=min_value(Features,all_dset,dset)\n",
    "Features=max_abs_value(Features,all_dset,dset)\n",
    "Features=abs_mean_derivate(Features,all_dset,dset)\n",
    "Features=max_abs_derivate(Features,all_dset,dset)\n",
    "Features=max_value_derivate(Features,all_dset,dset)\n",
    "Features=min_value_derivate(Features,all_dset,dset)\n",
    "Features=freq_max_power(Features,all_dset,dset)\n",
    "Features=freq_max_value(Features,all_dset,dset)\n",
    "Features=peak(Features,all_dset,dset)\n",
    "Features=fish_info_feat(Features,col_eeg_oxy,dset)\n",
    "Features= bin_power_features(Features, lst_data, dset)\n",
    "Features=freq_max_power_periodogram(Features, lst_data, dset)\n",
    "Features=puissance_moy_periodogram(Features, lst_data, dset)\n",
    "Features=max_amplitude_fft(Features, lst_data, dset)\n",
    "Features=freq_max_amplitude_fft(Features, lst_data, dset)\n",
    "Features=mean_amplitude_fft(Features, lst_data, dset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training/Testing\n",
    "\n",
    "Ensemble de cellules permettant d'entrainer le modèle sur une partie du Dataset pour le tester sur une autre .\n",
    "\n",
    "Afin de créer le Dataset, il y a deux possibilités : lire un csv ou recréer la Base de données\n",
    "\n",
    "Voir cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_train = pd.read_csv('Features_train_3.csv')\n",
    "Features_train=Features_train.iloc[:,2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv('train_y.csv').values[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Features_train.values, y, test_size = 0.25, random_state = 0)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Le Random Forest consiste en la création de plusieurs arbres de décision. Pour chaque évalutation, le résultat obtenu est le résultat obtenu par la majorité des arbres (ou une moyenne dans le cas d'une regression).\n",
    "Chaque arbre de décision peut être représenté par un ensemble de feuilles et noeuds. A chaque noeud, une condition portant sur les valeurs de notre échantillon enverra dans un des noeuds suivants et ainsi de suite.\n",
    "\n",
    "Lors de la phase d'entraînement, le modèle essaye de partitionner au mieux les données. Cela consiste en partitionner les données de façon à améliorer le critère de gini (plusieurs mesures possibles). L'arbre continue à créer des noeuds jusqu'à ce que le partitionnement de la donnée n'améliore plus le résultat ou qu'une seule classe soit atteinte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "multilabel-indicator is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-17-2070999be2ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mcm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_test_m\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mcm\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight)\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[0my_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    254\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"binary\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"multiclass\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 255\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"%s is not supported\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0my_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    256\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mlabels\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: multilabel-indicator is not supported"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    " \n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<28716x5 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 28716 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6083777290295623"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test,y_test_m)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG BOOST\n",
    "\n",
    "Cet algorithme (Extreme Gradient Boosting) est en fait un ensemble d'arbres de décision (Random Forest) couplé à un Booster de Gradient. Leurs structures sont très similaires. La seule véritable différence réside dans la façon avec laquelle le modèle est entraîné. Lorsqu'un nouvel arbre est créé, il vise à réduire les erreurs faites par les arbres précédents. Les arbres ont également moins de séparations que pour un Arbre décisionnel traditionnel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 593,   22,  176,   14,   95],\n",
       "       [  69,   20,  141,    8,   76],\n",
       "       [ 127,   37, 3506,  191,  454],\n",
       "       [  30,   11,  325, 1048,   26],\n",
       "       [  77,   28,  629,   29, 1841]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "classifier = XGBClassifier(max_depth=50, n_estimators=10, n_jobs=-1)\n",
    "classifier.fit(X_train,y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7320589157004074"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double split\n",
    "\n",
    "Pour ce modèle, nous avons séparé les données en deux parties : categories 2 et 4/ le reste.\n",
    "Cette idée est née de la proximité des données pour les catégories 2 et 4.\n",
    "Nous avons donc séparé les données grâce à un premier RandomForest. Puis nous avons classifié chaque partie séparemment grâce à deux nouveaux RandomForest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bin_cat(y_cat) : \n",
    "    y = y_cat\n",
    "    y[y == 1] = 0\n",
    "    y[y == 3] = 0\n",
    "    y[y == 2] = 1\n",
    "    y[y == 4] = 1\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_doub = bin_cat(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1750,  904],\n",
       "       [ 376, 6543]], dtype=int64)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(Features_train.values, y_doub, test_size = 0.25, random_state = 0)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "X_test=sc.transform(X_test)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model.fit(X_train,y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8662906090044918"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_1 = model.predict(Features_train.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_train['y_pred_1'] = y_pred_1\n",
    "Features_train['y'] = y\n",
    "Train_1 = Features_train.loc[Features_train['y_pred_1']==1,:]\n",
    "Train_0 = Features_train.loc[Features_train['y_pred_1']==0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 59,  50],\n",
       "       [ 28, 156]], dtype=int64)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RD Forest train0\n",
    "\n",
    "X_train_0, X_test_0, y_train_0, y_test_0 = train_test_split(Train_0.iloc[:,:-2].values,Train_0.iloc[:,-1], test_size = 0.25, random_state = 0)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train_0=sc.fit_transform(X_train_0)\n",
    "X_test_0=sc.transform(X_test_0)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_0 = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model_0.fit(X_train_0,y_train_0)\n",
    "y_pred_0 = model.predict(X_test_0)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_0 = confusion_matrix(y_test_0, y_pred_0)\n",
    "cm_0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7337883959044369"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.score(X_test_0,y_test_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1738,  875],\n",
       "       [ 613, 6054]], dtype=int64)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#RD Forest train0\n",
    "X_train_1, X_test_1, y_train_1, y_test_1 = train_test_split(Train_1.iloc[:,:-2].values,Train_1.iloc[:,-1], test_size = 0.25, random_state = 0)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train_1=sc.fit_transform(X_train_1)\n",
    "X_test_1=sc.transform(X_test_1)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_1 = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model_1.fit(X_train_1,y_train_1)\n",
    "y_pred_1 = model.predict(X_test_1)\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "cm_1 = confusion_matrix(y_test_1, y_pred_1)\n",
    "cm_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8671336206896552"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_1.score(X_test_1,y_test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[ 4106,  6597],\n",
       "       [ 8631, 18955]], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Train_0['y_pred_f'] = model_0.predict(Train_0.iloc[:,:-2].values)\n",
    "Train_1['y_pred_f'] = model_1.predict(Train_1.iloc[:,:-2].values)\n",
    "\n",
    "Res = pd.concat([Train_0,Train_1])\n",
    "cm_f = confusion_matrix(Res['y'],Res['y_pred_f'])\n",
    "cm_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Fold Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Afin de tester les résultats de notre modèle, nous avons utilisé la fonction cross_val_score. Cela permet de découper notre Training set en 10 parties égales, d'entraîner le modèle sur 9 parties et de le tester sur le dernière. Cela nous permet d'avoir une valeur moyenne (ainsi que la variance) du modèle. Cela permet de facilement souligner un possible overfitting, tout en ayant une bonne estimation statistique de notre modèle.\n",
    "\n",
    "Nous avons deux versions : une avec le score total et une avec le f1_score, valeur sur laquelle nous sommes testés sur le Kaggle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.75 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "Feat_tot=Features_train\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_kfold = StandardScaler()\n",
    "Feat_tot = sc_kfold.fit_transform(Feat_tot)\n",
    "\n",
    "## Model \n",
    "classifier = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "\n",
    "scores = cross_val_score(classifier, Feat_tot, y, cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest + one Hot Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n",
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\_encoders.py:385: DeprecationWarning: The 'categorical_features' keyword is deprecated in version 0.20 and will be removed in 0.22. You can use the ColumnTransformer instead.\n",
      "  \"use the ColumnTransformer instead.\", DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.62 (+/- 0.02)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "ohe = OneHotEncoder(categorical_features=[0])\n",
    "y_mult = ohe.fit_transform(y.reshape((-1,1))).toarray()\n",
    "\n",
    "Feat_tot=Features_train\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_kfold = StandardScaler()\n",
    "Feat_tot = sc_kfold.fit_transform(Feat_tot)\n",
    "\n",
    "## Model \n",
    "classifier = RandomForestClassifier(n_estimators=100, n_jobs=-1,)\n",
    "\n",
    "scores = cross_val_score(classifier, Feat_tot, y_mult, cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.78 (+/- 0.01)\n"
     ]
    }
   ],
   "source": [
    "Feat_tot=Features_train\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_kfold = StandardScaler()\n",
    "Feat_tot = sc_kfold.fit_transform(Feat_tot)\n",
    "\n",
    "## Model \n",
    "classifier = XGBClassifier(max_depth=50, n_estimators=100, n_jobs=-1)\n",
    "\n",
    "scores = cross_val_score(classifier, Feat_tot, y, cv=10)\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Target is multiclass but average='binary'. Please choose another average setting.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-64ad75815b44>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mclassifier\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m100\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m \u001b[0mscores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mFeat_tot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'f1'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Accuracy: %0.2f (+/- %0.2f)\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    403\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[0mreturn_times\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreturn_estimator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreturn_estimator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    239\u001b[0m             error_score=error_score)\n\u001b[1;32m--> 240\u001b[1;33m         for train, test in cv.split(X, y, groups))\n\u001b[0m\u001b[0;32m    241\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    242\u001b[0m     \u001b[0mzipped_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mscores\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    981\u001b[0m             \u001b[1;31m# remaining jobs.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    982\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 983\u001b[1;33m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    984\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    985\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    823\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 825\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    826\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    827\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 782\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    783\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    180\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    181\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 182\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    183\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    543\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    546\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 261\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    259\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m             return [func(*args, **kwargs)\n\u001b[1;32m--> 261\u001b[1;33m                     for func, args, kwargs in self.items]\n\u001b[0m\u001b[0;32m    262\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    263\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mfit_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m         \u001b[1;31m# _score will return dict if is_multimetric is True\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m         \u001b[0mtest_scores\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m         \u001b[0mscore_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mstart_time\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mfit_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mreturn_train_score\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer, is_multimetric)\u001b[0m\n\u001b[0;32m    603\u001b[0m     \"\"\"\n\u001b[0;32m    604\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mis_multimetric\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 605\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_multimetric_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    606\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    607\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_multimetric_score\u001b[1;34m(estimator, X_test, y_test, scorers)\u001b[0m\n\u001b[0;32m    633\u001b[0m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    634\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 635\u001b[1;33m             \u001b[0mscore\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    636\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    637\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'item'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\scorer.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m             return self._sign * self._score_func(y_true, y_pred,\n\u001b[1;32m---> 98\u001b[1;33m                                                  **self._kwargs)\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mf1_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m    718\u001b[0m     return fbeta_score(y_true, y_pred, 1, labels=labels,\n\u001b[0;32m    719\u001b[0m                        \u001b[0mpos_label\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 720\u001b[1;33m                        sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    721\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    722\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mfbeta_score\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m    832\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    833\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'f-score'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 834\u001b[1;33m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    835\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    836\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[0;32m   1045\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1046\u001b[0m             raise ValueError(\"Target is %s but average='binary'. Please \"\n\u001b[1;32m-> 1047\u001b[1;33m                              \"choose another average setting.\" % y_type)\n\u001b[0m\u001b[0;32m   1048\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mpos_label\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1049\u001b[0m         warnings.warn(\"Note that pos_label (set to %r) is ignored when \"\n",
      "\u001b[1;31mValueError\u001b[0m: Target is multiclass but average='binary'. Please choose another average setting."
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "Feat_tot=Features_train\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_kfold = StandardScaler()\n",
    "Feat_tot = sc_kfold.fit_transform(Feat_tot)\n",
    "\n",
    "## Model \n",
    "classifier = RandomForestClassifier(n_estimators=100, n_jobs=-1)\n",
    "\n",
    "scores = cross_val_score(classifier, Feat_tot, y, cv=10, scoring = 'f1')\n",
    "print(\"Accuracy: %0.2f (+/- %0.2f)\" % (scores.mean(), scores.std() * 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training sans test\n",
    "\n",
    "Ensemble de cellules permettant d'entrainer le modèle sur  Dataset pour le tester sur une autre \n",
    "\n",
    "Voir cross_validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_pred = h5py.File('test.h5', 'r')\n",
    "y_train=pd.read_csv('train_y.csv').values[:,1]\n",
    "dset = h5py.File('train.h5', 'r')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### From already preprocessed features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_train = pd.read_csv('Features_train_4.csv')\n",
    "Features_train = Features_train.iloc[:,2:]\n",
    "Features_test = pd.read_csv('Features_test_4.csv')\n",
    "Features_test = Features_test.iloc[:,2:]\n",
    "y_train = pd.read_csv('train_y.csv').values[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_train = Features_train.loc[:,:].values\n",
    "X_pred = Features_test.loc[:,:].values\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_pred = sc.transform(X_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model training\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_pred)\n",
    "Res=pd.DataFrame()\n",
    "Res['sleep_stage']=y_pred\n",
    "Res.index.name='id'\n",
    "Res.to_csv('y_pred.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### XG BOOST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=100, min_child_weight=1, missing=None, n_estimators=200,\n",
       "       n_jobs=-1, nthread=None, objective='multi:softprob', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "classifier_xg = XGBClassifier(max_depth=100, n_estimators=200, n_jobs=-1)\n",
    "classifier_xg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = classifier_xg.predict(X_pred)\n",
    "Res=pd.DataFrame()\n",
    "Res['sleep_stage']=y_pred\n",
    "Res.index.name='id'\n",
    "Res.to_csv('y_pred_xg.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "frequency=500\n",
    "duration =5000\n",
    "winsound.Beep(frequency,duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Random_forrest_Dreem.joblib']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from joblib import dump, load\n",
    "dump(model, 'Random_forest_Dreem.joblib') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Threshold ML\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "Features_train = pd.read_csv('Features_train_2.csv') \n",
    "Features_train = Features_train.iloc[:,1:]\n",
    "Features_test = pd.read_csv('Features_test_2.csv')\n",
    "Features_test = Features_test.iloc[:,1:]\n",
    "y_train = pd.read_csv('train_y.csv').iloc[:,[1]] #DF (WITH index)\n",
    "y_train['y_cat'] = bin_cat(y_train.values.copy())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Separating Sleep Stages 0,1,3 from 2,4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = Features_train.loc[:,:].values\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "X_train=sc.fit_transform(X_train)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_bin = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model_bin.fit(X_train,y_train.iloc[:,1].values)\n",
    "\n",
    "\n",
    "#y_train['y_pred_ts'] = y_pred = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extrating Sleep stages 2,4 and categorizing them again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\preprocessing\\data.py:617: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.partial_fit(X, y)\n",
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\sklearn\\base.py:462: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  return self.fit(X, **fit_params).transform(X)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_1 = Features_train.loc[y_train['y_cat']==1,:]\n",
    "y_train_1 = y_train.loc[y_train['y_cat']==1,'sleep_stage'].values\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc_2=StandardScaler()\n",
    "X_train_1=sc_2.fit_transform(X_train_1)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_2_4 = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model_2_4 .fit(X_train_1,y_train_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest on the whole Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
       "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=-1,\n",
       "            oob_score=False, random_state=None, verbose=0,\n",
       "            warm_start=False)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_2 = Features_train.loc[:,:].values\n",
    "y_train_2 = y_train.iloc[:,0].values\n",
    "X_train_2 = sc.transform(X_train_2)\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "model_glob = RandomForestClassifier(n_estimators=100,n_jobs=-1)\n",
    "model_glob.fit(X_train_2,y_train_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Test set prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\programdata\\anaconda3\\envs\\tensorflow\\lib\\site-packages\\ipykernel_launcher.py:10: DataConversionWarning: Data with input dtype int64, float64 were all converted to float64 by StandardScaler.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    }
   ],
   "source": [
    "X_test = Features_test.loc[:,:].values\n",
    "X_test = sc.transform(X_test)\n",
    "y_test = pd.DataFrame()\n",
    "y_test['y_glob'] = model_glob.predict(X_test)\n",
    "y_test['y_cat'] = model_bin.predict(X_test) \n",
    "\n",
    "\n",
    "\n",
    "X_test_1 = Features_test.loc[y_test['y_cat']==1,:]\n",
    "X_test_1=sc_2.transform(X_test_1)\n",
    "y_int = y_test.loc[y_test['y_cat']==1,['y_cat']]\n",
    "y_int['y_bin'] = model_2_4.predict(X_test_1)\n",
    "\n",
    "\n",
    "\n",
    "y_test = y_test.merge(y_int.loc[:,['y_bin']],right_index = True, left_index = True, how='outer')\n",
    "\n",
    "\n",
    "y_test['sleep_stage'] = y_test['y_bin'].fillna(y_test['y_glob'])\n",
    "\n",
    "y_test.index.name='id'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.loc[y_test['sleep_stage']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test['sleep_stage'] = y_test['sleep_stage'].astype('int64')\n",
    "y_test.loc[:,['sleep_stage']].to_csv('y_pred_triple.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Replacing values for 2,4 by the second result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
